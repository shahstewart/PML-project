---
title: Model selection for project 'Predicting Barbell Lift Quality From Accelerometer Data'
author: Sangeeta Shah
date: Oct 24, 2021
output:
    html_document:
        keep_md: yes
---

```{r echo=F, output=F, warning=F, message=F, error=F}
library(knitr)
opts_chunk$set(cache=T, warning=F, message=F, error=F)
```

&nbsp;  

This page contains the test models, the code and the model selection process details for the accompanying project
[_**Predicting Barbell Lift Quality from Accelerometer Data**_][1] 

&nbsp;  

## Data preprocessing
```{r}
library(caret); library(dplyr)

trainingSet <- read.csv('pml-training.csv'); testingSet <- read.csv('pml-testing.csv')

# remove irrelevant columns
trainingSet <- trainingSet[, -(1:7)]; testingSet <- testingSet[, -(1:7)]

# remove columns with missing / invalid data
badCols <- colSums(is.na(trainingSet) | trainingSet == '' | trainingSet == '#DIV/0!')
badCols <- badCols[badCols > (dim(trainingSet)[1] * .95)]
library(dplyr)
trainingSet <- trainingSet %>% select(-all_of(names(badCols)))
testingSet <- testingSet %>%  select(-all_of(names(badCols)))

# remove highly correlated columns
library(caret)
collinear <- findCorrelation(cor(trainingSet[, -53]), cutoff=0.95)
trainingSet <- trainingSet[, -collinear]; testingSet <- testingSet[, -collinear]


# factor the output column
trainingSet$classe <- factor(trainingSet$classe)

## divide the data into training and validation sets
set.seed(345325)
inTrain <- createDataPartition(y=trainingSet$classe, p=0.80, list=F)
trainingSet <- trainingSet[inTrain,]; validationSet <- trainingSet[-inTrain, ]

```

&nbsp;  

## Test Models

### Random Forests

#### Random Forests [ntree: 100, metric:Accuracy]
```{r}
set.seed(3333)
seedV <- vector(mode = 'list', length=2); seedV[[1]] <- sample.int(n=1000, 7); seedV[[2]] <- sample.int(n=1000, 1)
tuningGrid <- data.frame(mtry=6:12)

startTime <- proc.time()
mrf100 <- train(classe ~ ., data=trainingSet, method='rf', nodesize=5, 
                  metric='Accuracy', importance=T,
                  trControl= trainControl(method='oob', allowParallel=T, seeds=seedV),
                  tuneGrid= tuningGrid, ntree=100,
                  allowParallel=T)
mrf100time <- proc.time() - startTime
mrf100
```  

&nbsp;  


#### Random Forests [ntree: 150, metric:Accuracy]
```{r}
set.seed(3333)
startTime <- proc.time()
mrf150 <- train(classe ~ ., data=trainingSet, method='rf', nodesize=5, 
                  metric='Accuracy', importance=T,
                  trControl= trainControl(method='oob', allowParallel=T, seeds=seedV),
                  tuneGrid= tuningGrid, ntree=150,
                  allowParallel=T)
mrf150time <- proc.time() - startTime
mrf150
```  
&nbsp;  

#### Random Forests [ntree: 200, metric:Accuracy]
```{r}
set.seed(3333)
startTime <- proc.time()
mrf200 <- train(classe ~ ., data=trainingSet, method='rf', nodesize=5, 
                  metric='Accuracy', importance=T,
                  trControl= trainControl(method='oob', allowParallel=T, seeds=seedV),
                  tuneGrid= tuningGrid, ntree=200,
                  allowParallel=T)
mrf200time <- proc.time() - startTime
mrf200
```  


&nbsp;  

### Gradient Boosting 
#### [interaction depth: 1:3, ntree: 50:150, shrinkage: 0.1]

```{r}
tuningGrid <- expand.grid(
    interaction.depth= 1:3,
    n.trees = (1:3)*50,
    shrinkage = .1,
    n.minobsinnode = 10)

set.seed(3333)
seedV <- vector(mode = 'list', length=51); 
for(i in 1:50) seedV[[i]] <- sample.int(n=1000, 400); seedV[[51]] <- sample.int(n=1000, 1)

startTime <- proc.time()
mgbm <- train(classe ~ ., data=trainingSet, method='gbm', metric='Accuracy', 
                   trControl= trainControl(method='repeatedcv', repeats=5, allowParallel=T, seeds=seedV),
                   tuneGrid=tuningGrid,  verbose=F)
mgbmTime <- proc.time() - startTime
mgbm
```

## Testing the Models
**Model mrf100**
```{r}
mrf100Preds <- predict(mrf100$finalModel, validationSet)
crf100 <- confusionMatrix(mrf100Preds, validationSet$classe)
mrf100Accuracy <- round(sum(mrf100Preds == validationSet$classe) / nrow(validationSet), digits= 2)
mrf100Error <- 1 - mrf100Accuracy
crf100
```
&nbsp;  
**Model mrf150**
```{r}
mrf150Preds <- predict(mrf150$finalModel, validationSet)
crf150 <- confusionMatrix(mrf150Preds, validationSet$classe)
mrf150Accuracy <- round(sum(mrf150Preds == validationSet$classe) / nrow(validationSet), digits= 2)
mrf150Error <- 1 - mrf150Accuracy
crf150
```
&nbsp;    
**Model mrf200**
```{r}
mrf200Preds <- predict(mrf200$finalModel, validationSet)
crf200 <- confusionMatrix(mrf200Preds, validationSet$classe)
mrf200Accuracy <- round(sum(mrf200Preds == validationSet$classe) / nrow(validationSet), digits= 2)
mrf200Error <- 1 - mrf200Accuracy
crf200
```
&nbsp;  

**Model mgmb**
```{r}
mgbmPreds <- predict(mgbm, validationSet)
cmgbm <- confusionMatrix(mgbmPreds, validationSet$classe)
mgbmAccuracy <- round(sum(mgbmPreds == validationSet$classe) / nrow(validationSet), digits= 2)
mgbmError <- 1 - mgbmAccuracy
cmgbm
```  

&nbsp;  

## Selecting the model
```{r}
mbest <- mgbm$bestTune
mrf100M <- c(Algorithm = 'Random Forest', ntree = 100, mtry = mrf100$bestTune['mtry'], shrinkage = NA,
             interaction.depth = NA, Metric = 'Accuracy', Accuracy = '99.46%',
             Compute.Time = round(mrf100time['elapsed'], 2), Validation.Accuracy = round(mrf100Accuracy, 2),
             Misclassification.Error = round(mrf100Error, 2))
mrf150M <- c(Algorithm = 'Random Forest', ntree = 150, mtry = mrf150$bestTune['mtry'], shrinkage = NA,
             interaction.depth = NA, Metric = 'Accuracy', Accuracy = '99.48%',
             Compute.Time = mrf150time['elapsed'], Validation.Accuracy = round(mrf150Accuracy, 2),
             Misclassification.Error = round(mrf150Error, 2))
mrf200M <- c(Algorithm = 'Random Forest', ntree = 200, mtry = mrf200$bestTune['mtry'], shrinkage = NA,
             interaction.depth = NA, Metric = 'Accuracy', Accuracy = '99.50%', 
             Compute.Time = mrf200time['elapsed'], Validation.Accuracy = round(mrf200Accuracy, 2),
             Misclassification.Error = round(mrf200Error, 2))
mgbmM <-   c(Algorithm = 'Gradient Boosting', ntree = mbest['n.trees'], mtry = NA, shrinkage = mbest['shrinkage'],
             interaction.depth = mbest['interaction.depth'], Metric = 'Accuracy', Accuracy = '96.00%',
             Compute.Time = mgbmTime['elapsed'], Validation.Accuracy = round(mgbmAccuracy, 2),
             Misclassification.Error = round(mgbmError, 2))

models <- cbind(mrf100M, mrf150M, mrf200M, mgbmM)
colnames(models) <- c('Rf ntree 100', 'Rf ntree 150', 'Rf ntree 200', 'Gradient Boosting')
kable(models)
```
** In the table above, while _Accuracy_ is the accuracy of the model as provided by the train function,
_Validation.Accuracy_ is calculated based on predictions obtained on the validation set.


[1]: project.html  
